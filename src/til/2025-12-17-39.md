---
title: 로드 밸런싱
description: 로드 밸런싱은 무엇인가요?
date: 2025-12-17
tags: [로드 밸런싱]
---

로드 밸런싱은 네트워크 또는 서버에 들어오는 트래픽 부하를 여러 대의 서버에 균등하게 나누어 분산시키는 기술 또는 행위를 말합니다.

이는 시스템의 성능과 안정성을 향상시키기 위한 분산 처리 아키텍처의 핵심 구성 요소입니다.

## 1. 로드 밸런싱의 핵심 목표

로드 밸런싱을 사용하는 주된 이유는 다음과 같습니다.

1. 확장성: 단일 서버의 처리 능력 한계를 극복하고, 수평적 확장(Scale-out, 서버 대수를 늘리는 것)을 통해 시스템의 처리 용량을 무한히 늘릴 수 있게 합니다.
2. 가용성: 특정 서버 한 대가 다운되더라도, 로드 밸런서가 이를 감지하고 해당 서버로의 트래픽 전송을 중단하여 서비스의 연속성을 보장합니다.
3. 성능 최적화: 모든 요청이 균등하게 분산되므로, 모든 사용자가 일관되고 빠른 응답 속도를 경험할 수 있게 합니다.

## 2. 로드 밸런싱의 동작 방식

로드 밸런서는 클라이언트와 서버 사이에 위치하며, 다음과 같은 방식으로 작동합니다.

1. 요청 수신: 로드 밸런서의 가상 IP 주소로 들어오는 모든 클라이언트 요청을 받습니다.
2. 분산 알고리즘 적용: 미리 정의된 부하 분산 알고리즘(예: Round Robin, Least Connection)을 적용하여, 현재 부하가 가장 적거나 적합한 서버를 선택합니다.
3. 트래픽 전달: 선택된 서버로 요청을 전달합니다.
4. 헬스 체크: 로드 밸런서는 주기적으로 백엔드 서버들의 상태를 체크합니다. 응답이 없거나 비정상적인 서버는 자동으로 트래픽 분배 목록에서 제외합니다.

## 3. 주요 로드 밸런싱 알고리즘

- 라운드 로빈
  - 서버들에게 순차적으로 (1,2,3,1,2,3,…) 트래픽을 분산합니다.
  - 가장 단순하고 공평한 방식 (서버들의 성능이 동일할 때)
- 가중치 라운드 로빈
  - 서버의 처리 용량(성능)에 가중치를 두어, 성능이 좋은 서버에 더 많은 트래픽을 분배합니다.
  - 서버 성능이 다를 때
- 최소 연결
  - 현재 활성 연결 수가 가장 적은 서버로 요청을 보냅니다.
  - 세션 시간이 긴 서비스에 유리
- IP 해시
  - 클라이언트의 IP 주소를 해싱하여 특정 서버에 매핑합니다.
  - 사용자가 항상 같은 서버에 접속하도록 보장

## 4. OSI 7계층 기반 분류 (L4 vs L7)

로드 밸런서가 OSI 7계층 중 어떤 계층의 정보를 활용하여 트래픽을 분산시키느냐에 따라 L4와 L7으로 구분됩니다.

| 구분      | L4 로드 밸런서                  | L7 로드 밸런서                             |
| --------- | ------------------------------- | ------------------------------------------ |
| 분산 기준 | 전송 계층 (Layer 4)             | 애플리케이션 계층 (Layer 7)                |
| 활용 정보 | IP 주소, Port 번호              | URL, HTTP 헤더, 쿠키 내용                  |
| 동작 방식 | 패킷 레벨에서 처리 (단순)       | HTTP 요청 내용을 분석 (지능적)             |
| 장점      | 처리 속도가 빠르고 부하가 적음  | 특정 서비스나 API별로 분산 가능            |
| 단점      | 단순 분산만 가능                | 패킷 내용을 봐야 해서 부하가 큼            |
| 예시      | AWS NLB (Network Load Balancer) | Nginx, AWS ALB (Application Load Balancer) |

실무 적용 예시

- `/api/v1/user` 요청은 서버 그룹 A로,
- `/api/v2/image` 요청은 서버 그룹 B로 분산시키는 것은 L7 로드 밸런서만이 가능합니다.

## 정리

로드 밸런싱은 다수의 서버를 활용하여 읽기/쓰기 부하를 균등하게 분산시키고, 서버 장애 시 자동으로 트래픽을 우회시켜 시스템의 가용성과 처리량을 극대화하는 필수 아키텍처 기술입니다.
